{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install yfinance\n",
    "!pip install arch\n",
    "!pip install keras-tuner\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GARCH\n",
    "from arch import arch_model\n",
    "\n",
    "# LSTM & Deep Learning con TensorFlow Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import Huber  # Huber Loss per robustezza agli outlier\n",
    "\n",
    "# Scaling e Metriche\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Hyperparameter tuning con KerasTuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "##########################################\n",
    "# Callback per fermare il training se compare NaN\n",
    "##########################################\n",
    "class CheckNaNCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if np.isnan(logs.get('loss', 0)) or np.isnan(logs.get('val_loss', 0)):\n",
    "            print(\"NaN loss rilevato, interrompo il trial.\")\n",
    "            self.model.stop_training = True\n",
    "            raise ValueError(\"NaN loss encountered.\")\n",
    "            # Non lanciamo l'eccezione; il trial terminerà e il tuner registrerà il risultato\n",
    "\n",
    "##########################################\n",
    "# 1. Caricamento dei dati e preprocessing\n",
    "##########################################\n",
    "def load_data(tickers, start_date='01/01/2007', end_date='23/12/2024',\n",
    "              save_dir='/content/drive/MyDrive/Prova_Garch'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    start_date_dt = pd.to_datetime(start_date, format='%d/%m/%Y')\n",
    "    end_date_dt = pd.to_datetime(end_date, format='%d/%m/%Y')\n",
    "    data_frames = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            print(f\"Elaborazione ticker: {ticker}\")\n",
    "            data = yf.download(ticker, period='max', auto_adjust=False)\n",
    "            data = data.sort_index().ffill().bfill()\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                data.columns = [' '.join(col).strip() for col in data.columns]\n",
    "            data.columns = [col.replace(f\" {ticker}\", \"\") for col in data.columns]\n",
    "            print(f\"Colonne disponibili per {ticker}: {data.columns}\")\n",
    "            data = data[(data.index >= start_date_dt) & (data.index <= end_date_dt)]\n",
    "            data = data[data['Volume'] > 0]\n",
    "            data['return'] = data['Adj Close'].pct_change() * 100  # percentuale\n",
    "            data['log_return'] = np.log(data['Adj Close'] / data['Adj Close'].shift(1))\n",
    "            data_frames[ticker] = data\n",
    "            filename = os.path.join(save_dir, f\"{ticker}_data.csv\")\n",
    "            data.to_csv(filename, float_format='%.3f', index=True)\n",
    "            print(f\"Dati salvati in: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore per il ticker {ticker}: {e}\")\n",
    "    return data_frames\n",
    "\n",
    "##########################################\n",
    "# 2. Feature Engineering: aggiunta di feature extra\n",
    "##########################################\n",
    "def compute_RSI(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def compute_MACD(series, short_window=12, long_window=26, signal_window=9):\n",
    "    ema_short = series.ewm(span=short_window, adjust=False).mean()\n",
    "    ema_long = series.ewm(span=long_window, adjust=False).mean()\n",
    "    macd_line = ema_short - ema_long\n",
    "    signal_line = macd_line.ewm(span=signal_window, adjust=False).mean()\n",
    "    hist = macd_line - signal_line\n",
    "    return macd_line, signal_line, hist\n",
    "\n",
    "def compute_bollinger_bands(series, window=20, num_std=2):\n",
    "    sma = series.rolling(window=window).mean()\n",
    "    rolling_std = series.rolling(window=window).std()\n",
    "    upper_band = sma + (rolling_std * num_std)\n",
    "    lower_band = sma - (rolling_std * num_std)\n",
    "    return sma, upper_band, lower_band\n",
    "\n",
    "def compute_stochastic_oscillator(data, k_window=14, d_window=3):\n",
    "    low_min = data['Low'].rolling(k_window).min()\n",
    "    high_max = data['High'].rolling(k_window).max()\n",
    "    stoch_k = 100 * ((data['Close'] - low_min) / (high_max - low_min))\n",
    "    stoch_d = stoch_k.rolling(d_window).mean()\n",
    "    return stoch_k, stoch_d\n",
    "\n",
    "def add_features(data):\n",
    "    if 'log_return' not in data.columns:\n",
    "        data['log_return'] = np.log(data['Adj Close'] / data['Adj Close'].shift(1))\n",
    "    data['gk_vol'] = np.sqrt(\n",
    "        0.5 * (np.log(data['High'] / data['Low'])) ** 2 -\n",
    "        (2 * np.log(2) - 1) * (np.log(data['Close'] / data['Open'])) ** 2\n",
    "    )\n",
    "    data['rolling_vol'] = data['log_return'].rolling(window=10).std()\n",
    "    data['realized_vol'] = data['log_return'].rolling(window=5).std()\n",
    "    data['log_realized_vol'] = np.log(data['realized_vol'] + 1e-6)\n",
    "    data['target_vol'] = data['log_realized_vol'].shift(-1)\n",
    "    data['lag_log_return'] = data['log_return'].shift(1)\n",
    "    data['lag_realized'] = data['realized_vol'].shift(1)\n",
    "    data['RSI'] = compute_RSI(data['Adj Close'])\n",
    "    data['SMA'] = data['Adj Close'].rolling(window=14).mean()\n",
    "    sma_20, bb_up, bb_down = compute_bollinger_bands(data['Adj Close'], window=20, num_std=2)\n",
    "    data['BB_mid'] = sma_20\n",
    "    data['BB_up'] = bb_up\n",
    "    data['BB_down'] = bb_down\n",
    "    macd_line, signal_line, hist = compute_MACD(data['Adj Close'])\n",
    "    data['MACD_line'] = macd_line\n",
    "    data['MACD_signal'] = signal_line\n",
    "    data['MACD_hist'] = hist\n",
    "    stoch_k, stoch_d = compute_stochastic_oscillator(data)\n",
    "    data['Stoch_K'] = stoch_k\n",
    "    data['Stoch_D'] = stoch_d\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "##########################################\n",
    "# 3. Ottimizzazione del modello GARCH e forecasting a blocchi (ricorsivo)\n",
    "##########################################\n",
    "def grid_search_garch(sample_data, p_values, q_values, dists, model_type, criterion='aic'):\n",
    "    best_crit_value = np.inf\n",
    "    best_params = None\n",
    "    best_dist = None\n",
    "    best_o = 0\n",
    "    if model_type == 'GJR':\n",
    "        vol_type = 'Garch'\n",
    "        o_param = 1\n",
    "    elif model_type == 'TARCH':\n",
    "        vol_type = 'aparch'\n",
    "        o_param = 0\n",
    "    else:\n",
    "        vol_type = model_type\n",
    "        o_param = 0\n",
    "    for p in p_values:\n",
    "        for q in q_values:\n",
    "            for dist in dists:\n",
    "                try:\n",
    "                    if o_param > 0:\n",
    "                        am = arch_model(sample_data, vol=vol_type, p=p, o=o_param, q=q, dist=dist)\n",
    "                    elif model_type == 'TARCH':\n",
    "                        am = arch_model(sample_data, vol='aparch', p=p, q=q, dist=dist, power=1.0)\n",
    "                    else:\n",
    "                        am = arch_model(sample_data, vol=vol_type, p=p, q=q, dist=dist)\n",
    "                    res = am.fit(update_freq=0, disp='off')\n",
    "                    crit_value = res.aic if criterion == 'aic' else res.bic\n",
    "                    if crit_value < best_crit_value:\n",
    "                        best_crit_value = crit_value\n",
    "                        best_params = (p, q)\n",
    "                        best_dist = dist\n",
    "                        best_o = o_param\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return best_params, best_dist, best_crit_value, best_o\n",
    "\n",
    "def optimize_garch_models(data, p_values=[1,2,3], q_values=[1,2,3], dists=['t','skewt','ged'],\n",
    "                          windows=[250,500,750,1000,1500], criterion='aic', ticker=None):\n",
    "    log_ret = data['log_return'].dropna() * 100\n",
    "    best_overall = np.inf\n",
    "    best_model_type = None\n",
    "    best_model_params = None\n",
    "    best_model_dist = None\n",
    "    best_window = None\n",
    "    model_list = ['Garch','EGARCH','GJR','TARCH']\n",
    "    for w in windows:\n",
    "        sample_data = log_ret.iloc[-w:]\n",
    "        for m in model_list:\n",
    "            params, dist, crit_val, _ = grid_search_garch(sample_data, p_values, q_values, dists, m, criterion)\n",
    "            print(f\"Modello {m}: best_params={params}, dist={dist}, {criterion.upper()}={crit_val:.4f}\")\n",
    "            if params is not None and crit_val < best_overall:\n",
    "                best_overall = crit_val\n",
    "                best_model_type = m\n",
    "                best_model_params = params\n",
    "                best_model_dist = dist\n",
    "                best_window = w\n",
    "    print(f\"Selezionato {best_model_type} con p={best_model_params[0]}, q={best_model_params[1]}, \"\n",
    "          f\"dist={best_model_dist}, window={best_window} e {criterion.upper()}={best_overall:.4f}\")\n",
    "\n",
    "    # Forecasting ricorsivo in blocchi di 10 giorni (1-step alla volta)\n",
    "    forecast_vol = pd.Series(index=log_ret.index, dtype=float)\n",
    "    step = 10\n",
    "    n = len(log_ret)\n",
    "    pos = best_window  # partiamo dopo la finestra ottimale\n",
    "    while pos < n:\n",
    "        block_size = min(step, n - pos)\n",
    "        block_forecasts = []\n",
    "        for j in range(block_size):\n",
    "            current_window = log_ret.iloc[pos - best_window: pos]\n",
    "            try:\n",
    "                if best_model_type == 'GJR':\n",
    "                    am = arch_model(current_window, vol='Garch', p=best_model_params[0], o=1, q=best_model_params[1],\n",
    "                                    dist=best_model_dist)\n",
    "                elif best_model_type == 'TARCH':\n",
    "                    am = arch_model(current_window, vol='aparch', p=best_model_params[0], q=best_model_params[1],\n",
    "                                    dist=best_model_dist, power=1.0)\n",
    "                else:\n",
    "                    am = arch_model(current_window, vol=best_model_type, p=best_model_params[0], q=best_model_params[1],\n",
    "                                    dist=best_model_dist)\n",
    "                res = am.fit(update_freq=0, disp='off')\n",
    "                fc = res.forecast(horizon=1)\n",
    "                forecast_value = np.sqrt(fc.variance.iloc[-1, 0])\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel forecasting a partire da indice {pos}: {e}\")\n",
    "                forecast_value = np.nan\n",
    "            block_forecasts.append(forecast_value)\n",
    "            pos += 1\n",
    "        indices_block = log_ret.index[pos - block_size: pos]\n",
    "        forecast_vol.loc[indices_block] = block_forecasts\n",
    "\n",
    "    # Se ci sono NaN, applichiamo un forward-fill\n",
    "    forecast_vol = forecast_vol.fillna(method='ffill')\n",
    "    # Clipping per evitare valori estremi nella feature forecast\n",
    "    forecast_vol = forecast_vol.clip(lower=0, upper=np.percentile(forecast_vol.dropna(), 99))\n",
    "\n",
    "    data = data.copy()\n",
    "    data['garch_vol_forecast'] = np.nan\n",
    "    common_idx = data.index.intersection(forecast_vol.index)\n",
    "    data.loc[common_idx, 'garch_vol_forecast'] = forecast_vol.loc[common_idx]\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "##########################################\n",
    "# 4. Preparazione dei dati per il modello LSTM\n",
    "##########################################\n",
    "def prepare_lstm_data_multivariate(data, feature_cols, target_col, look_back=10):\n",
    "    df = data.copy().dropna()\n",
    "    X, y, dates = [], [], []\n",
    "    features = df[feature_cols].values\n",
    "    target = df[target_col].values\n",
    "    for i in range(look_back, len(df)):\n",
    "        X.append(features[i-look_back:i])\n",
    "        y.append(target[i])\n",
    "        dates.append(df.index[i])\n",
    "    return np.array(X), np.array(y), np.array(dates)\n",
    "\n",
    "def split_data_by_date(X, y, dates, test_period_years=5):\n",
    "    last_date = pd.to_datetime(dates.max())\n",
    "    test_start_date = last_date - pd.DateOffset(years=test_period_years)\n",
    "    train_mask = dates < test_start_date\n",
    "    test_mask = dates >= test_start_date\n",
    "    return X[train_mask], y[train_mask], X[test_mask], y[test_mask], dates[train_mask], dates[test_mask]\n",
    "\n",
    "def scale_data(X_train, X_val, X_test):\n",
    "    ns, ts, nf = X_train.shape\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(-1, nf)\n",
    "    X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
    "    X_train_scaled = X_train_scaled_flat.reshape(ns, ts, nf)\n",
    "\n",
    "    ns_val = X_val.shape[0]\n",
    "    X_val_flat = X_val.reshape(-1, nf)\n",
    "    X_val_scaled = scaler.transform(X_val_flat).reshape(ns_val, ts, nf)\n",
    "\n",
    "    ns_test = X_test.shape[0]\n",
    "    X_test_flat = X_test.reshape(-1, nf)\n",
    "    X_test_scaled = scaler.transform(X_test_flat).reshape(ns_test, ts, nf)\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "# Funzione di default per hyperparameters (fallback)\n",
    "def default_hyperparameters(num_features, look_back=10):\n",
    "    hp = kt.HyperParameters()\n",
    "    hp.Fixed('look_back', look_back)\n",
    "    hp.Fixed('num_features', num_features)\n",
    "    hp.Int('units_1', 32, 128, step=32, default=64)\n",
    "    hp.Float('dropout_1', 0.2, 0.4, step=0.1, default=0.3)\n",
    "    hp.Int('units_2', 32, 128, step=32, default=64)\n",
    "    hp.Float('dropout_2', 0.1, 0.3, step=0.1, default=0.1)\n",
    "    hp.Boolean('use_third_layer', default=False)\n",
    "    hp.Float('learning_rate', 1e-4, 3e-4, sampling='log', default=1.5e-4)\n",
    "    return hp\n",
    "\n",
    "##########################################\n",
    "# 5. Costruzione del modello LSTM\n",
    "##########################################\n",
    "# Modifica dei model builder per ridurre instabilità numerica\n",
    "\n",
    "def advanced_model_builder(hp, look_back=10, num_features=15):\n",
    "    reg = l2(1e-4)\n",
    "    units_1 = hp.Int('units_1', 32, 256, step=32, default=64)\n",
    "    dropout_1 = hp.Float('dropout_1', 0.1, 0.5, step=0.1, default=0.3)\n",
    "    units_2 = hp.Int('units_2', 32, 256, step=32, default=64)\n",
    "    dropout_2 = hp.Float('dropout_2', 0.1, 0.5, step=0.1, default=0.1)\n",
    "    use_third_layer = hp.Boolean('use_third_layer', default=False)\n",
    "    if use_third_layer:\n",
    "        units_3 = hp.Int('units_3', 32, 256, step=32, default=64)\n",
    "        dropout_3 = hp.Float('dropout_3', 0.1, 0.5, step=0.1, default=0.1)\n",
    "    learning_rate = hp.Float('learning_rate', 1e-5, 3e-4, sampling='log', default=1e-4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units_1, return_sequences=True, input_shape=(look_back, num_features), kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(LSTM(units_2, return_sequences=use_third_layer, kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout_2))\n",
    "    if use_third_layer:\n",
    "        model.add(LSTM(units_3, kernel_regularizer=reg))\n",
    "        model.add(Dropout(dropout_3))\n",
    "    model.add(Dense(1, kernel_regularizer=reg))\n",
    "    opt = Adam(learning_rate=learning_rate, clipvalue=0.5)\n",
    "    model.compile(optimizer=opt, loss=Huber())\n",
    "    return model\n",
    "\n",
    "def advanced_model_builder_fallback(hp, look_back=10, num_features=15):\n",
    "    reg = l2(1e-4)\n",
    "    units_1 = hp.Int('units_1', 32, 128, step=32, default=64)\n",
    "    dropout_1 = hp.Float('dropout_1', 0.1, 0.3, step=0.1, default=0.2)\n",
    "    units_2 = hp.Int('units_2', 32, 128, step=32, default=64)\n",
    "    dropout_2 = hp.Float('dropout_2', 0.1, 0.2, step=0.1, default=0.1)\n",
    "    learning_rate = 1e-4\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units_1, return_sequences=True, input_shape=(look_back, num_features), kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(LSTM(units_2, kernel_regularizer=reg))\n",
    "    model.add(Dropout(dropout_2))\n",
    "    model.add(Dense(1, kernel_regularizer=reg))\n",
    "    opt = Adam(learning_rate=learning_rate, clipvalue=0.5)\n",
    "    model.compile(optimizer=opt, loss=Huber())\n",
    "    return model\n",
    "\n",
    "# Alias per il builder standard\n",
    "advanced_model_builder_standard = advanced_model_builder\n",
    "\n",
    "##########################################\n",
    "# Funzioni di training: standard e fallback\n",
    "##########################################\n",
    "def train_with_tuner(ticker, X_train, y_train, X_val, y_val, conservative=False, max_epochs=40):\n",
    "    check_nan_cb = CheckNaNCallback()\n",
    "    if conservative:\n",
    "        builder = advanced_model_builder_fallback\n",
    "        dir_name = f\"./kt_dir_fallback\"\n",
    "        proj_name = f\"fallback_{ticker}\"\n",
    "    else:\n",
    "        builder = advanced_model_builder_standard\n",
    "        dir_name = f\"./kt_dir_standard\"\n",
    "        proj_name = f\"standard_{ticker}\"\n",
    "    try:\n",
    "        tuner = kt.Hyperband(\n",
    "            lambda hp: builder(hp, look_back=X_train.shape[1], num_features=X_train.shape[2]),\n",
    "            objective='val_loss',\n",
    "            max_epochs=max_epochs,\n",
    "            factor=3,\n",
    "            directory=dir_name,\n",
    "            project_name=proj_name,\n",
    "            max_consecutive_failed_trials=20\n",
    "        )\n",
    "        tuner.search(\n",
    "            X_train, y_train,\n",
    "            epochs=max_epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[check_nan_cb],\n",
    "            verbose=1\n",
    "        )\n",
    "        best_hp_list = tuner.get_best_hyperparameters(num_trials=1)\n",
    "        if not best_hp_list:\n",
    "            print(f\"[{ticker}] Nessun iperparametro valido trovato, uso default.\")\n",
    "            return default_hyperparameters(X_train.shape[2], X_train.shape[1])\n",
    "        return best_hp_list[0]\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Tuner search fallito con errore: {e}. Uso default hyperparameters.\")\n",
    "        return default_hyperparameters(X_train.shape[2], X_train.shape[1])\n",
    "\n",
    "def train_lstm_for_ticker(ticker, X_train, y_train, X_val, y_val, max_epochs=40):\n",
    "    try:\n",
    "        print(f\"[{ticker}] Provo pipeline standard.\")\n",
    "        best_hp = train_with_tuner(ticker, X_train, y_train, X_val, y_val, conservative=False, max_epochs=max_epochs)\n",
    "        print(f\"[{ticker}] Pipeline standard completata. Parametri: {best_hp.values}\")\n",
    "        used_fallback = False\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Pipeline standard fallita: {e}\")\n",
    "        print(f\"[{ticker}] Uso pipeline fallback.\")\n",
    "        best_hp = train_with_tuner(ticker, X_train, y_train, X_val, y_val, conservative=True, max_epochs=max_epochs)\n",
    "        print(f\"[{ticker}] Pipeline fallback completata. Parametri: {best_hp.values}\")\n",
    "        used_fallback = True\n",
    "    return best_hp, used_fallback\n",
    "\n",
    "##########################################\n",
    "# MAIN\n",
    "##########################################\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tickers = ['XLK', 'XLV', 'XLF', 'XLE', 'XLY', 'XLI', 'NVDA', 'LMT', 'WMT', 'XOM', 'NKE', 'AMZN', 'NFLX', 'AAPL']\n",
    "    data_dict = load_data(tickers)\n",
    "    for ticker in tickers:\n",
    "        print(f\"\n",
    "=== Processing {ticker} ===\")\n",
    "        data = data_dict[ticker].copy()\n",
    "        data = add_features(data)\n",
    "        # Applichiamo il modello GARCH per ottenere le previsioni in blocchi (ricorsivo 1-step)\n",
    "        data = optimize_garch_models(data, ticker=ticker)\n",
    "\n",
    "        feature_cols = [\n",
    "            'garch_vol_forecast', 'gk_vol', 'rolling_vol', 'log_return',\n",
    "            'lag_log_return', 'RSI', 'SMA', 'BB_mid', 'BB_up', 'BB_down',\n",
    "            'MACD_line', 'MACD_signal', 'MACD_hist', 'Stoch_K', 'Stoch_D'\n",
    "        ]\n",
    "        target_col = 'target_vol'\n",
    "        look_back = 10\n",
    "\n",
    "        X, y, dates = prepare_lstm_data_multivariate(data, feature_cols, target_col, look_back=look_back)\n",
    "        X_train, y_train, X_test, y_test, train_dates, test_dates = split_data_by_date(X, y, dates, test_period_years=5)\n",
    "\n",
    "        train_split = int(0.8 * len(X_train))\n",
    "        X_train_final, y_train_final = X_train[:train_split], y_train[:train_split]\n",
    "        X_val, y_val = X_train[train_split:], y_train[train_split:]\n",
    "\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled, scalerX = scale_data(X_train_final, X_val, X_test)\n",
    "\n",
    "        scalerY = StandardScaler()\n",
    "        y_train_scaled = scalerY.fit_transform(y_train_final.reshape(-1,1)).ravel()\n",
    "        y_val_scaled = scalerY.transform(y_val.reshape(-1,1)).ravel()\n",
    "        y_test_scaled = scalerY.transform(y_test.reshape(-1,1)).ravel()\n",
    "        y_train_scaled = np.clip(y_train_scaled, -3, 3)\n",
    "        y_val_scaled = np.clip(y_val_scaled, -3, 3)\n",
    "        y_test_scaled = np.clip(y_test_scaled, -3, 3)\n",
    "\n",
    "        # Verifica eventuali NaN nei dati scalati\n",
    "        print(\"X_train_scaled NaNs:\", np.isnan(X_train_scaled).sum())\n",
    "        print(\"y_train_scaled NaNs:\", np.isnan(y_train_scaled).sum())\n",
    "\n",
    "        num_features = X_train_scaled.shape[2]\n",
    "\n",
    "        best_hp, used_fallback = train_lstm_for_ticker(ticker, X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, max_epochs=40)\n",
    "        best_hp_dict = best_hp.values\n",
    "        print(\"Best Hyperparameters:\", best_hp_dict)\n",
    "\n",
    "        if used_fallback:\n",
    "            final_model = advanced_model_builder_fallback(best_hp, look_back, num_features)\n",
    "        else:\n",
    "            final_model = advanced_model_builder_standard(best_hp, look_back, num_features)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "        nan_cb = CheckNaNCallback()\n",
    "\n",
    "        try:\n",
    "            history = final_model.fit(\n",
    "                X_train_scaled, y_train_scaled,\n",
    "                epochs=best_hp_dict.get('tuner/epochs', 40),\n",
    "                batch_size=16,\n",
    "                validation_data=(X_val_scaled, y_val_scaled),\n",
    "                callbacks=[es, rlrop, nan_cb],\n",
    "                verbose=1\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"[{ticker}] Training finale fallito con NaN loss: {e}\")\n",
    "            print(f\"[{ticker}] Riparto in modalità fallback con training semplificato.\")\n",
    "            final_model = advanced_model_builder_fallback(best_hp, look_back, num_features)\n",
    "            # Riparto con un numero minore di epoche per stabilizzare\n",
    "            history = final_model.fit(\n",
    "                X_train_scaled, y_train_scaled,\n",
    "                epochs=20,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_val_scaled, y_val_scaled),\n",
    "                callbacks=[es, rlrop],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "        test_loss = final_model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "        print(f\"[{ticker}] Test Loss (MSE in log-scale): {test_loss:.4f}\")\n",
    "\n",
    "        y_pred_scaled = final_model.predict(X_test_scaled)\n",
    "        y_pred_log = scalerY.inverse_transform(y_pred_scaled)\n",
    "        y_pred = np.exp(y_pred_log) - 1e-6\n",
    "\n",
    "        y_test_log = scalerY.inverse_transform(y_test_scaled.reshape(-1,1))\n",
    "        real_vol_test = np.exp(y_test_log) - 1e-6\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(real_vol_test, y_pred))\n",
    "        mae = mean_absolute_error(real_vol_test, y_pred)\n",
    "        print(f\"[{ticker}] RMSE (scala reale): {rmse:.6f}, MAE (scala reale): {mae:.6f}\")\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'Date': test_dates,\n",
    "            'Volatilità Reale (exp)': real_vol_test.flatten(),\n",
    "            'Volatilità Predetta (exp)': y_pred.flatten()\n",
    "        })\n",
    "        results_csv_path = os.path.join('/content/drive/MyDrive/Prova_Garch/', f\"risultati_forecasting_{ticker}.csv\")\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "        print(f\"[{ticker}] Risultati salvati in: {results_csv_path}\")\n",
    "\n",
    "        history_csv_path = os.path.join('/content/drive/MyDrive/Prova_Garch/', f\"training_history_{ticker}.csv\")\n",
    "        pd.DataFrame(history.history).to_csv(history_csv_path, index=False)\n",
    "        model_save_path = os.path.join('/content/drive/MyDrive/Prova_Garch/', f\"modello_lstm_{ticker}.h5\")\n",
    "        final_model.save(model_save_path)\n",
    "        print(f\"[{ticker}] Modello LSTM salvato in: {model_save_path}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(real_vol_test, label='Volatilità Reale (exp)')\n",
    "        plt.plot(y_pred, label='Volatilità Predetta (exp)')\n",
    "        plt.title(f\"Forecasting della Volatilità per {ticker} (ultimi 5 anni) - fallback={used_fallback}\")\n",
    "        plt.xlabel(\"Campioni\")\n",
    "        plt.ylabel(\"Volatilità\")\n",
    "        plt.legend()\n",
    "        plot_path = os.path.join('/content/drive/MyDrive/Prova_Garch/', f\"Reale_vs_Pred_{ticker}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show() "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
